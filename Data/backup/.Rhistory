library(MASS)
library(leaps)
library(datasets)
library(devtools)
install_github('vqv/ggbiplot')
install.packages('ggbiplot')
install.packages("remotes")
install.packages("remotes")
remotes::install_github("vqv/ggbiplot")
install.packages('installr')
library(installr)
update()
update()
updateR()
remotes::install_github("vqv/ggbiplot")
remotes::install_github("vqv/ggbiplot")
library(MASS)
library(leaps)
library(datasets)
library(devtools)
library(ggbiplot)
library(usethis)
library(devtools)
library(ggplot2)
library(plyr)
library(scales)
library(grid)
library(ggbiplot)
#### Variable subset selection
data(cpus)
?cpus
#remove 'name' and 'estperf' variables
cpus$name <- NULL
cpus$estperf <- NULL
# Exhaustive
regfit_full_exhaustive <- regsubsets(perf ~ ., data = cpus, method = 'exhaustive')
subset_summary_exhaustive <- summary(regfit_full_exhaustive)
# Backward - starts with all - eliminates one by one the ones that are useless
regfit_full_backward <- regsubsets(perf ~ ., data = cpus, method = 'backward')
subset_summary_backward <- summary(regfit_full_backward)
# Forward - starts with none - adds one by one the most usefull
regfit_full_forward <- regsubsets(perf ~ ., data = cpus, method = 'forward')
subset_summary_forward <- summary(regfit_full_forward)
subset_summary_exhaustive, subset_summary_backward, subset_summary_forward
subset_summary_exhaustive
subset_summary_backward
subset_summary_forward
subset_summary_exhaustive$outmat
subset_summary_backward$outmat
subset_summary_forward$outmat
# Compare results by the adjusted R^2
plot(subset_summary_backward$adjr2,
type = 'o',
col = 'red',
ylab = 'adjusted R2',
xlab = 'Number of variables')
lines(subset_summary_exhaustive$adjr2,
type = 'o',
col = 'blue')
lines(subset_summary_forward$adjr2,
type = 'o',
col = 'green')
legend('bottomright', col = c('blue', 'red', 'green'),
legend = c('Exhaustive', 'Backward', 'Forward'),
lty = 1, pch = 1)
# Compare results by the adjusted R^2
plot(subset_summary_backward$adjr2,
type = 'o',
col = 'red',
ylab = 'adjusted R2',
xlab = 'Number of variables')
lines(subset_summary_exhaustive$adjr2,
type = 'o',
col = 'blue')
lines(subset_summary_forward$adjr2,
type = 'o',
col = 'green')
# Compare results by the adjusted R^2
plot(subset_summary_backward$adjr2,
type = 'o',
col = 'red',
ylab = 'adjusted R2',
xlab = 'Number of variables')
lines(subset_summary_exhaustive$adjr2,
type = 'o',
col = 'blue')
#### Principal Components Analysis
# Load data
data(iris)
pca_iris <- prcomp(iris[, -5], scale = T)
pca_iris
summary(pca_iris)
#visualize the different projections of the 2 first principal components
ggbiplot(pca_iris)
library(MASS)
data('Pima.tr')
#Visualize dataset for classification exercise
pairs(Pima.tr[, -ncol(Pima.tr)], col = Pima.tr$type) # remove the last column and set colour to the type
#Distribution of the 2 classes
barplot(table(Pima.tr$type), main = 'Class Distribution')
#### Logistic regression
# Fitting model
glm_fit_all <- glm(type ~ . , data = Pima.tr, family = 'binomial')
#Predict the output
glm_probs <- predict(glm_fit, Pima.tr, type = 'response') #type = 'response' gives the probabilities
glm_probs[1:10] #Display first 10
glm_predictions <- rep('No', nrow(Pima.tr))
glm_predictions[glm_probs > 0.5] <- 'Yes'
#Predict the output
glm_probs <- predict(glm_fit_all, Pima.tr, type = 'response') #type = 'response' gives the probabilities
glm_predictions <- rep('No', nrow(Pima.tr))
glm_predictions[glm_probs > 0.5] <- 'Yes'
glm_predictions[1:10]
#Confusion matrix
glm_matrix <- table(glm_predictions, Pima.tr$type)
glm_matrix
#Accuracy
glm_accuracy <- sum(diag(glm_matrix))/sum(glm_matrix)
#Sensitivity
glm_sensit <- glm_matrix[2,2]/sum(glm_matrix[,2])
#### LDA
# Fitting model
lda_fit <- lda(type ~ glu + ped , data = Pima.tr)
lda_fit
#### LDA
# Fitting model
lda_fit <- lda(type ~ . , data = Pima.tr)
#Predict the output
lda_probs <- predict(lda_fit, Pima.tr)
#Confusion matrix
lda_matrix <- table(lda_probs$class, Pima.tr$type)
lda_matrix
#### QDA
# Fitting model
qda_fit <- qda(type ~ . , data = Pima.tr)
#Predict the output
qda_probs <- predict(qda_fit, Pima.tr)
#Confusion matrix
qda_matrix <- table(qda_probs$class, Pima.tr$type)
qda_matrix
glm_probs[1:10] #Display first 10
#Predict the output
lda_probs <- predict(lda_fit, Pima.tr)
lda_probs
lda_probs[1:10]
# Plotting libraries
library(ggplot2)
library(ggbiplot)
library(corrplot)
library(caret) # Cross-Validation
library(e1071) # SVM model
library(Metrics) # RSME metric
library(Rtools)
install.packages(Rtools)
install.packages(Rtools)
install.packages('Rtool')
install.packages('Rtools')
library(swirl)
install.packages("swirl()")
library(swirl)
install.packages("swirl")
library(swirl)
install_from_swirl("R Programming")
swirl()
6+7
5+7
x <- 5+7
x
y <- x - 3
y
z <- c(1.1, 9, 3.14)
?c
z
C(z, 555)
setwd(E:/facultate/Data Science/Data Mining/Labs/Lab 1)
setwd(E:\facultate\Data Science\Data Mining\Labs\Lab 1)
# Create dataframe courses
course <- c('Data Science', 'Cyber Security', 'IT', 'Digital media')
module1 <- c('analytics', 'ethical hacking', 'databases', '3D animation')
credits1 <- c(15, 15, 20, 30)
module2 <- c('project', 'forensics', 'Java programming', 'perception')
credits2 <- c(45, 15, 20, 12)
courses <- data.frame(course, module1, credits1, module2, credits2, stringsAsFactors = T)
# Extend the 'courses' with a row
nrow <- c('Networks', 'CISCO CCNA', 25, 'Operating systems', 40)
courses <- rbind(courses, nrow)
courses <- data.frame(course, module1, credits1, module2, credits2, stringsAsFactors = T)
nrow <- as.data.frame(nrow)
courses <- rbind(courses, nrow)
View(nrow)
View(courses)
# Extend the 'courses' with a row
nrow <- c('Networks', 'CISCO CCNA', 25, 'Operating systems', 40)
nrowdf <- as.data.frame(nrow)
courses <- rbind(courses, nrow)
###### Data Frames
#Define vectors to put them in the data frame
stIds <- c(1:5)
stNames <- c("St1", "St2", "St3", "St4", "St5")
stGrades <- c("Excellent", "Good", "Bad", "Really Bad", "Really Really Bad")
#create a data frame to combine the above vectors
df <- data.frame(student_id = stIds, student_name = stNames, student_grades = stGrades)
names(df) #gives the names of the collumns of the data frame
#Add a new row on dataframe
newStudent <- c(6, "St6", "Bad", 55.6)
df<- df[-c(6:12),] #fixing the dataframe (this is not part of adding the row) -> deleting the rows
df <- rbind(df, newStudent)
print(df)
courses <- rbind(courses, nrowdf)
View(nrowdf)
View(df)
#nrowdf <- as.data.frame(nrow)
courses <- rbind(courses, nrow)
courses <- data.frame(course, module1, credits1, module2, credits2, stringsAsFactors = F)
# Extend the 'courses' with a row
nrow <- c('Networks', 'CISCO CCNA', 25, 'Operating systems', 40)
#nrowdf <- as.data.frame(nrow)
courses <- rbind(courses, nrow)
# Extend the 'courses' with a column 'total number of credits'
ncol <- courses[, 3] + courses[, 5]
courses[1, 3]
?as.factor
# Extend the 'courses' with a column 'total number of credits'
ncol <- as.numeric(courses[, 3]) + as.numeric(courses[, 5])
# Extend the 'courses' with a column 'total number of credits'
ncol <- as.numeric(courses$credits1) + as.numeric(courses$credits2)
courses$total <- as.numeric(courses$credits1) + as.numeric(courses$credits2)
courses$total <- courses$credits1 + courses$credits2
# Extend the 'courses' with a boolean column 'pgcert' with T = >60 credits
courses$pgcert <- if(courses$total >= 60, T)
# Extend the 'courses' with a boolean column 'pgcert' with T = >60 credits
courses$pgcert <- ifelse(courses$total >= 60, T,
ifelse(courses$total < 60, F))
# Extend the 'courses' with a boolean column 'pgcert' with T = >60 credits
courses$pgcert <- ifelse(courses$total >= 60, 'T',
ifelse(courses$total < 60, 'F'))
# Extend the 'courses' with a boolean column 'pgcert' with T = >60 credits
courses$pgcert <- ifelse(courses$total >= 60, 'T', 'F')
# Create new dataframe with credits of module >= 20
portofolio <- subset(courses, courses$credits1 >= 20 | courses$credits2 >= 20)
View(portofolio)
# Create new dataframe with no databases courses
nodb <- subset(courses, courses$module1 != 'databases' | courses$module2 != 'databases')
View(nodb)
# Create new dataframe with no databases courses
nodb <- subset(courses, courses$module1 != 'databases' & courses$module2 != 'databases')
missDiabetes[sample(1:nrow(missDiabetes), 0.2 * nrow(missDiabetes)), 'age'] <- NA
# Libraries
library(caret) # has kNN implementations
library(mlbench) # has the PimaIndiansDiabetes dataset
library(e1071)
library(partykit)
library(Hmisc)
library(kknn)
library(snn)
data("PimaIndiansDiabetes")
### kNN
# Leave one out evaluation
ctrl1 <- trainControl(method = 'LOOCV')
set.seed(123)
# Use kNN on the dataset
mod11.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
trControl = ctrl1)
# Check accuracy for k values
print(mod11.knn)  # best accuracy for k = 9 with the value 0.7343750
# Visualise accuracy for various k values
plot(mod11.knn, type = 'p')
## Confusion matrix
# Get the results of best k that are stored in 'pred'
results <- (mod11.knn$pred[mod11.knn$pred$k ==
mod11.knn$bestTune$k,])
# Remove unwated columns (k and row index)
results$k <- NULL
results$rowIndex <- NULL
# Produce confusion matrix
table(results)
# Testing different k values
set.seed(123)
mod21.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
tuneGrid = expand.grid(.k = 1:20),  # Different values for k
trControl = ctrl1)
print(mod21.knn) # The best accuracy is for k = 19 with value 0.7604
plot(mod21.knn, type = 'p')
### Exercise 1
# 3 repeats of 10-fold CV evaluation
ctrl2 <- trainControl(method = 'repeatedcv',
number = 10,
repeats = 3)
set.seed(123)
mod31.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
tuneGrid = expand.grid(.k = 1:20),
trControl = ctrl2)
print(mod31.knn) # best accuracy is for k = 19 with value 0.7539702
plot(mod31.knn, type = 'p')
# Bootstrap
ctrl3 <- trainControl(method = 'boot')
set.seed(123)
mod41.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
tuneGrid = expand.grid(.k = 1:20),
trControl = ctrl3)
print(mod41.knn) # best accuracy is for k = 16 with value of 0.7364233
plot(mod41.knn, type = 'p')
### Using only selected values of k (21 and 23)
mod51.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
tuneGrid = expand.grid(k = c(21,23)),
trControl = ctrl1)
print(mod51.knn) # Best k is 21 with accuracy 0.7578125
plot(mod51.knn, type = 'p')
### Pre-processing numeric values
summary(PimaIndiansDiabetes)
# Normalisation
set.seed(123)
mod61.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
tuneGrid = expand.grid(.k = 1:20),
preProcess = c('range'),
trControl = ctrl1)
print(mod61.knn)
plot(mod61.knn, type = 'p')
# Standardising
set.seed(123)
mod71.knn <- train(diabetes ~.,
data = PimaIndiansDiabetes,
method = 'knn',
preProcess = c('center', 'scale'),
trControl = ctrl1)
print(mod71.knn)
### Checking the pre-processed datasets
# Normalised data
preProcessedNorm <- preProcess(PimaIndiansDiabetes, method = c('range'))
diabetesNorm <- predict(preProcessedNorm, PimaIndiansDiabetes)
summary(diabetesNorm)
# Standardised data
preProcessedStand <- preProcess(PimaIndiansDiabetes, method = c('center', 'scale'))
diabetesStand <- predict(preProcessedStand, PimaIndiansDiabetes)
summary(diabetesStand)
### Nominal to binary - one-hot encoding
# make a copy of WeatherPlay dataset
noClass <- WeatherPlay # WeatherPlay is in partykit library
# remove the class - will not be transformed
noClass$play <- NULL
set.seed(123)
# binarise nominal attributes - one-hot encoding
binaryVars <- dummyVars(~.,
data = noClass)
newWeather <- predict(binaryVars, newdata = noClass)
# add the class to the binarised dataset
binarisedWeather <- cbind(newWeather, WeatherPlay[5])
### Missing values
# Introducing missing values
set.seed(123)
# make copy of the dataset
missDiabetes <- PimaIndiansDiabetes
# sample 30 values from 'glucose' column and turn them into NA
missDiabetes[sample(1:nrow(missDiabetes), 30), 'glucose'] <- NA
summary(missDiabetes)
# Replacing 20% of values with missing values for age column
set.seed(123)
missDiabetes[sample(1:nrow(missDiabetes), 0.2 * nrow(missDiabetes)), 'age'] <- NA
# Loading the "swPollution" dataset
swPollution <- read.csv('swPollution.csv', stringsAsFactors = T)
# Loading the "swPollution" dataset
swPollution <- read.csv('swPollution.csv', stringsAsFactors = T)
# Preparing the dataset
swPollution$Year <- NULL
swPollution$Facility <- NULL
swPollution$Easting <- NULL
swPollution$Northing <- NULL
# Pre-processing the dataset
# Currently, the attribute 'Region' has 12 values that some overlap and they belong to the same Region in Scotland
# Using the list of Scotland regions to lower the number of different values for the attribute 'Region'
swPollution$Region[swPollution$Region == 'MidEdinburgh and Lothians'] <- 'Edinburgh and Lothians'
swPollution$Region[swPollution$Region == 'Argyll & Bute'] <- 'Glasgow and Strathclyde'
# Change the values 'Perth & Kinross', 'Tayside, Central and Tayside, Central and Fife' to 'Tayside, Central and Fife'
swPollution$Region[swPollution$Region == "Perth & Kinross"] <- "Tayside, Central and Fife"
swPollution$Region[swPollution$Region == "Tayside, Central and Tayside, Central and Fife"] <- "Tayside, Central and Fife"
swPollution$Region <- droplevels(swPollution$Region)
summary(swPollution$Region)
# Set seed
set.seed(123)
#Caret for createDataPartition
library(caret)
# Create a data partion containing 10% of the whole dataset
swp10 <- createDataPartition(y = swPollution$Medium,
p = .1,
list = FALSE)
swp10Entries <- swPollution[swp10,]
# Obtain 50% of swp10Entries
# Set seed
set.seed(123)
# Create data partition
swp5 <- createDataPartition(y = swp10Entries$Medium,
p = .5,
list = FALSE)
swpTrain <- swp10Entries[swp5,]
swpTest <- swp10Entries[-swp5,]
summary(swpTrain)
# Setting the validation control - 3 repeats of 8-fold cross validation
ctrl <- trainControl(method = "repeatedcv",
number = 8,
repeats = 3,
verboseIter = FALSE)
# Training CART model on swpTrain
set.seed(123)
CART_model_swpTrain <- train(Medium ~.,
data = swpTrain,
method = 'rpart',
trControl = ctrl)
# Training C5.0 model on swpTrain
set.seed(123)
C50_model_swpTrain <- train(Medium ~.,
data = swpTrain,
method = 'C5.0',
trControl = ctrl)
# Training kNN model on swpTrain
set.seed(123)
kNN_model_swpTrain <- train(Medium ~.,
data = swpTrain,
method = 'knn',
tuneGrid = expand.grid(k = c(1, 3, 5, 7)),
trControl = ctrl)
# Comparing the models
# Collecting the results
results_swpTrain <- resamples(list(CART = CART_model_swpTrain, C50 = C50_model_swpTrain, kNN = kNN_model_swpTrain))
results_swpTrain
summary(results_swpTrain) #optional
setwd('E:/#facultate/Data Science/Project/Data')
setwd('E:/facultate/Data Science/Project/Data')
fault_data <- read.csv('fault_data.csv')
scada_data <- read.csv('scada_data.csv')
status <- read.csv('status_data.csv')
View(fault_data)
View(scada_data)
View(status)
View(scada_data)
